# -*- coding: utf-8 -*-
"""Attentiveness_Detection_from_EEG_signal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UaTSNOxqg8EQASTHEjeQW-FSEbYwBvCF
"""

#importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
!pip install mne

from google.colab import drive
drive.mount('/content/drive')

import pickle
import mne
import matplotlib.pyplot as plt

filename = '/content/drive/MyDrive/thesis/data_allsubjects'

with open(filename, 'rb') as handle:
    data = pickle.load(handle)

print(len(data))

subject1 = data[0]
print(subject1)

""" Output:
<EpochsArray  |   3982 events (all good), 0 - 0.585938 sec, baseline off, ~37.0 MB, data loaded,
 'neg': 3413
 'pos': 569>
"""

# Extract target and nontarget samples as:
target = subject1['pos']
nonTarget = subject1['neg']

print(subject1.ch_names)
#df = np.squeeze(subject1.get_data(picks='pos'))

# Here data is a python list to access first subjects

subject1 = data[0]
print(subject1)

""" Output:
<EpochsArray  |   3982 events (all good), 0 - 0.585938 sec, baseline off, ~37.0 MB, data loaded,
 'neg': 3413
 'pos': 569>
"""

# Extract target and nontarget samples as:
target = subject1['pos']
nonTarget = subject1['neg']

# to get the numpy array:
numpy_array = subject1['pos'].get_data()
#print(target.get_data())

# Output:  numpy_array.shape()
#         trials [x] channels [x] samples

# ==============================
# Binary Classification setting:
# =============================
import numpy as np

# construct X,Y dataset for binary classification:

X, Y = [], []
X = np.concatenate([target.get_data(), nonTarget.get_data()])
Y = np.concatenate([np.ones(target.get_data().shape[0]), np.zeros(nonTarget.get_data().shape[0])])

# Y = 1 => Target Stimulus (a character/letter in the matrix that a user wants to spell)
# Y = 0 => Non-Target Stimulus
print(X.shape)
print(Y.shape)
plt.imshow(X[1])
plt.show()
print(Y)

print(X)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
X_train.shape
#y_train.shape

X_test.shape

#X.ndim
#X.shape
#Y.shape
#Y.ndim
#X
#Y
print(len(X_train))
X_train.shape

!pip install keras
from keras.callbacks import EarlyStopping
import tensorflow as tf
from keras import  layers, models
from keras.layers import Dropout
from tensorflow.keras.layers import Dense, BatchNormalization
import matplotlib.pyplot as plt

'''model = models.Sequential()
model.add(layers.Conv1D(32,(3) ,padding='same',activation='relu', input_shape=(16,76)))
model.add(layers.MaxPooling1D((2)))
model.add(layers.Conv1D(32,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(Dropout(0.6))
model.add(layers.Conv1D(64,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(Dropout(0.6))
model.add(layers.Conv1D(64,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(Dropout(0.6))
model.add(layers.Flatten())
model.add(layers.Dense(16, activation='relu'))
BatchNormalization(),
model.add(layers.Dense(32, activation='relu'))
BatchNormalization(),
model.add(layers.Dense(2, activation='relu'))
BatchNormalization(),
#model.add(Dropout(0.6))
model.add(layers.Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',
              loss="BinaryCrossentropy",
              metrics=['accuracy'])
model.summary()

model = models.Sequential()
model.add(layers.Conv1D(32,(3) ,padding='same',activation='relu', input_shape=(16,76)))
model.add(BatchNormalization())
model.add(layers.Conv1D(64,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(layers.Conv1D(128,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(BatchNormalization())
model.add(Dropout(0.6))
model.add(layers.Conv1D(64,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(layers.Conv1D(32,(3) ,padding='same',activation='relu'))
model.add(layers.MaxPooling1D((2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',
              loss="BinaryCrossentropy",
              metrics=['accuracy'])
model.summary()

history = model.fit(X_train, y_train, epochs=100,
                    validation_data=(X_test , y_test))

history.history.keys()

test_acc = history.history["accuracy"]
plt.plot(history.history['accuracy'], label='test_accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
'''plt.xlim([0.005, 1])
plt.ylim([0.005, 1])'''
plt.legend(loc='upper right')

test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)

plt.plot(history.history['accuracy'], label='test_accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper right')

test_loss = history.history["loss"]
val_loss = history.history["val_loss"]

plt.plot(history.history['loss'], label='test_loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
test_loss,test_acc= model.evaluate(X_test,y_test, verbose=2)

train_acc = history.history["accuracy"]
val_accuracy = history.history["val_accuracy"]

plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.legend(loc='upper right')

train_loss,train_acc = model.evaluate(X_train, y_train, verbose=2)

train_loss = history.history["loss"]
val_loss = history.history["loss"]

plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label = 'val_loss')
plt.xlabel('Epoch')
plt.ylabel('loss')

plt.legend(loc='upper right')

train_loss,train_acc = model.evaluate(X_train, y_train, verbose=2)

"""# Convert to 1D """

X.dtype

print(X.shape)
print(Y.shape)

X_1d = X.reshape(X.shape[0], (X.shape[1]*X.shape[2]))

print(X_1d.shape)

print(X_1d[0])

X_train2, X_test2, y_train2, y_test2 = train_test_split(X_1d, Y, test_size=0.2)

print(X_train2.shape)

"""**Logistic Regression**

"""

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train2, y_train2)
y_pred2 = clf.predict(X_test2)
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred4))
# evaluate predictions
predictions = [round(value) for value in y_pred2]

accuracy = accuracy_score(y_test2, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

from sklearn.metrics import classification_report 

target_names = ['Non-Target Stimulus', 'Target Stimulus']
print(classification_report(y_test2, y_pred2, target_names=target_names))

"""**Random Forest**"""

from sklearn.ensemble import RandomForestClassifier
rf_clf = RandomForestClassifier(criterion='entropy')   
rf_clf.fit(X_train2,y_train2)
y_pred3 = rf_clf.predict(X_test2)
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred3))
# evaluate predictions
predictions = [round(value) for value in y_pred3]

accuracy = accuracy_score(y_test2, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

from sklearn.metrics import classification_report 

target_names = ['Non-Target Stimulus', 'Target Stimulus']
print(classification_report(y_test2, y_pred3, target_names=target_names))

"""**AdaBoost**"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score
Adamodel =AdaBoostClassifier(n_estimators=100,learning_rate=1)
# Train Adaboost Classifer
model = Adamodel.fit(X_train2, y_train2)
#Predict the response for test dataset
y_pred4 = model.predict(X_test2)


# Model Accuracy, how often is the classifier correct?
#print("Accuracy:",metrics.accuracy_score(y_test, y_pred4))
# evaluate predictions
predictions = [round(value) for value in y_pred4]

accuracy = accuracy_score(y_test2, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

from sklearn.metrics import classification_report 

target_names = ['Non-Target Stimulus', 'Target Stimulus']
print(classification_report(y_test2, y_pred4, target_names=target_names))

"""**Xgb**"""

from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
model = XGBClassifier()
model.fit(X_train2, y_train2)
y_pred5 = model.predict(X_test2)
predictions = [round(value) for value in y_pred5]
# evaluate predictions
accuracy = accuracy_score(y_test2, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

from sklearn.metrics import classification_report 

target_names = ['Non-Target Stimulus', 'Target Stimulus']
print(classification_report(y_test2, y_pred5, target_names=target_names))

#model.add(layers.Conv1D(512, (3) , activation='relu'))
#model.add(layers.MaxPooling1D((1)))
#model.add(layers.Conv1D(128, (3) , activation='relu'))
#model.add(layers.MaxPooling1D((1)))
#model.add(layers.Conv1D(64, (3) , activation='relu'))
#model.add(layers.MaxPooling1D((1)))
#model.add(layers.Conv1D(32, (3) , activation='relu'))
#model.add(layers.MaxPooling1D((1)))